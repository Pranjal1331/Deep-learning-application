{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXaWfOQbYXJq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Final Assignment: Data driven approach to laminar and turbulent flow classification\n",
        "\n",
        "Laminar flow has been characterized for its smooth and linear behavior concerning its flow.\n",
        "When a non-linearity is introduced, the flow becomes turbulent.\n",
        "Visually it is possible to distinguish between laminar and turbulent flow by\n",
        "identifying areas in the flow that become chaotic or non-smooth.\n",
        "We used a Convolutional Neural Network (CNN) trained on labeled laminar and\n",
        "turbulent images to automate the process. In this assignment, we take simulated\n",
        "contours from a flow past cylinder simulation, train it, and test it to see how\n",
        "effective the CNN is at distinguishing between the laminar and turbulent flow.\n",
        "The geometry is as follows -\n",
        "There is a circular cylinder axis along the z-axis(out of the plane),\n",
        "you have to simulate 2d velocity profile about it.\n",
        "\n",
        "Objective of CNN model is that given a simulated velocity profile,\n",
        "it should be able to efficiently predict the nature of fluid flow.\n",
        "Compare the accuracy of different CNN architectures covered in the class and\n",
        "based on that choose a model of your dataset.\n",
        "Then optimize hyperparameters to get accuracy more than 80%\n",
        "\n",
        " Deadline to the assignment is : 10th May\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. create  and preprocess\n",
        "- 10 laminar/10 turbulent images for dataset\n",
        "- preprocess - resize, normalise\n",
        "2. try some cnns\n",
        "3. optimise hyperparameters (80% accuracy)\n",
        "4. documentation\n",
        "'''"
      ],
      "metadata": {
        "id": "QQ1FgHmoK4bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "3. Model Training and Evaluation:\n",
        "   - Train the selected CNN model on the training set.\n",
        "   - Validate the model on the testing set to evaluate its performance.\n",
        "   - Fine-tune the model to improve its accuracy.\n",
        "\n",
        "4. Hyperparameter Optimization:\n",
        "   - Tune hyperparameters such as learning rate, batch size, number of epochs, etc.\n",
        "   - Use techniques like grid search or random search to find optimal hyperparameters.\n",
        "   - Aim to achieve an accuracy of more than 80% as per the assignment requirements.\n",
        "\n",
        "6. Report and Documentation:\n",
        "   - Document your methodology, results, and observations.\n",
        "'''"
      ],
      "metadata": {
        "id": "zldcdYLZYxuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "proKANujdQti"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting a seed to ensure reproducibility\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "R5hK4J3BNW6b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU\n",
        "\n",
        "print(\"GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "laminar_dir = \"/content/drive/MyDrive/Fluid_flow_data/laminar\"\n",
        "laminar_dir_p = \"/content/drive/MyDrive/Fluid_flow_data/laminarp\"\n",
        "\n",
        "turbulant_dir = \"/content/drive/MyDrive/Fluid_flow_data/turbulent\"\n",
        "turbulant_dir_p = \"/content/drive/MyDrive/Fluid_flow_data/turbulantp\"\n",
        "\n",
        "#source_dir = \"/content/drive/MyDrive/Fluid_flow_data\"\n",
        "train_dir = \"/content/drive/MyDrive/Fluid_flow_data/train\"\n",
        "test_dir = \"/content/drive/MyDrive/Fluid_flow_data/test\"\n",
        "val_dir = \"/content/drive/MyDrive/Fluid_flow_data/validation\"\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "os.makedirs(laminar_dir_p, exist_ok=True)\n",
        "os.makedirs(turbulant_dir_p, exist_ok=True)\n",
        "\n",
        "lamtrain = os.path.join(train_dir, \"laminar\")\n",
        "lamtest = os.path.join(test_dir, \"laminar\")\n",
        "lamval = os.path.join(val_dir, \"laminar\")\n",
        "\n",
        "turtrain = os.path.join(train_dir, \"turbulant\")\n",
        "turtest = os.path.join(test_dir, \"turbulant\")\n",
        "turval = os.path.join(val_dir, \"turbulant\")\n",
        "\n",
        "os.makedirs(lamtrain, exist_ok=True)\n",
        "os.makedirs(lamtest, exist_ok=True)\n",
        "os.makedirs(lamval, exist_ok=True)\n",
        "os.makedirs(turtrain, exist_ok=True)\n",
        "os.makedirs(turtest, exist_ok=True)\n",
        "os.makedirs(turval, exist_ok=True)\n",
        "\n",
        "#train_dir = r\"/content/drive/MyDrive/Fluid_flow_data\"\n"
      ],
      "metadata": {
        "id": "bOHLMSm8NaIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c8c3b6-e011-48a2-c480-34100b653d90"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 150, 150\n",
        "num_classes = 2\n",
        "batchsize = 10"
      ],
      "metadata": {
        "id": "IWbKdk6_yquu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to preprocess images\n",
        "def preprocess_images(source_dir, target_dir):\n",
        "      for file in os.listdir(source_dir):\n",
        "          # load image\n",
        "          img = load_img(os.path.join(source_dir, file), target_size=(img_width, img_height))\n",
        "          img_array = img_to_array(img)\n",
        "          img_array /= 255.0  # normalize pixel values\n",
        "          # move preprocessed image to target directory\n",
        "          # os.rename(os.path.join(subdir, file), os.path.join(target_dir, subdir.split('/')[-1] + '_' + file))\n",
        "          os.rename(os.path.join(source_dir, file), os.path.join(target_dir, file))\n",
        "\n",
        "preprocess_images(laminar_dir, laminar_dir_p)\n",
        "preprocess_images(turbulant_dir, turbulant_dir_p)"
      ],
      "metadata": {
        "id": "E8evOYDyeolH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into train, test, and validation\n",
        "laminar_files = os.listdir(laminar_dir_p)\n",
        "turbulent_files = os.listdir(turbulant_dir_p)\n",
        "\n",
        "laminar_train, laminar_test_val = train_test_split(laminar_files, test_size=0.3, random_state=42)\n",
        "laminar_test, laminar_val = train_test_split(laminar_test_val, test_size=0.5, random_state=42)\n",
        "\n",
        "turbulent_train, turbulent_test_val = train_test_split(turbulent_files, test_size=0.3, random_state=42)\n",
        "turbulent_test, turbulent_val = train_test_split(turbulent_test_val, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "2axAVt-4w0Dk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def move_files(source_files, source_dir, dest_dir):\n",
        "    for file in source_files:\n",
        "        #os.rename(os.path.join(source_dir, file), os.path.join(dest_dir, file))\n",
        "        os.rename(os.path.join(source_dir, file), os.path.join(dest_dir, file))"
      ],
      "metadata": {
        "id": "PCyJimPk2me5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move files to appropriate directories\n",
        "\n",
        "move_files(laminar_train, laminar_dir_p, os.path.join(train_dir, \"laminar\"))\n",
        "move_files(laminar_test, laminar_dir_p, os.path.join(test_dir, \"laminar\"))\n",
        "move_files(laminar_val, laminar_dir_p, os.path.join(val_dir, \"laminar\"))\n",
        "\n",
        "move_files(turbulent_train, turbulant_dir_p, os.path.join(train_dir, \"turbulant\"))\n",
        "move_files(turbulent_test, turbulant_dir_p, os.path.join(test_dir, \"turbulant\"))\n",
        "move_files(turbulent_val, turbulant_dir_p, os.path.join(val_dir, \"turbulant\"))\n"
      ],
      "metadata": {
        "id": "Njx0_cfRnm1A"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "lamtrain = os.path.join(train_dir, \"laminar\")\n",
        "lamtest = os.path.join(test_dir, \"laminar\")\n",
        "lamval = os.path.join(val_dir, \"laminar\")\n",
        "\n",
        "turtrain = os.path.join(train_dir, \"turbulant\")\n",
        "turtest = os.path.join(test_dir, \"turbulant\")\n",
        "turval = os.path.join(val_dir, \"turbulant\")\n",
        "'''"
      ],
      "metadata": {
        "id": "mF2KOADFxRf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encoding\n",
        "# 0 - laminar\n",
        "# 1 - turbulent\n",
        "\n",
        "# train\n",
        "train_files = []\n",
        "train_files.extend([os.path.join(lamtrain, file) for file in laminar_train])\n",
        "train_files.extend([os.path.join(turtrain, file) for file in turbulent_train])\n",
        "\n",
        "train_labels = []\n",
        "train_labels.extend([0] * len(laminar_train))\n",
        "train_labels.extend([1] * len(turbulent_train))\n",
        "\n",
        "train_labels = to_categorical(train_labels, 2)\n",
        "\n",
        "# test\n",
        "test_files = []\n",
        "test_files.extend([os.path.join(lamtest, file) for file in laminar_test])\n",
        "test_files.extend([os.path.join(turtest, file) for file in turbulent_test])\n",
        "\n",
        "test_labels = []\n",
        "test_labels.extend([0] * len(laminar_test))\n",
        "test_labels.extend([1] * len(turbulent_test))\n",
        "\n",
        "test_labels = to_categorical(test_labels, 2)\n",
        "\n",
        "# validation\n",
        "val_files = []\n",
        "val_files.extend([os.path.join(lamval, file) for file in laminar_val])\n",
        "val_files.extend([os.path.join(turval, file) for file in turbulent_val])\n",
        "\n",
        "val_labels = []\n",
        "val_labels.extend([0] * len(laminar_val))\n",
        "val_labels.extend([1] * len(turbulent_val))\n",
        "\n",
        "val_labels = to_categorical(val_labels,2)\n"
      ],
      "metadata": {
        "id": "V1DPpQxHPoG3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the model\n",
        "\n",
        "model1 = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# compiling the model\n",
        "model1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "2i1oHYLbNgM_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess images\n",
        "train_images = []\n",
        "\n",
        "for file in train_files:\n",
        "\n",
        "    img = load_img(file, target_size=(img_width, img_height))\n",
        "    img_array = img_to_array(img)\n",
        "    train_images.append(img_array)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "val_images = []\n",
        "\n",
        "for file in val_files:\n",
        "\n",
        "    img = load_img(file, target_size=(img_width, img_height))\n",
        "    img_array = img_to_array(img)\n",
        "    val_images.append(img_array)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "val_images = np.array(val_images)\n",
        "val_labels = np.array(val_labels)\n"
      ],
      "metadata": {
        "id": "t-OWU7lM5gkg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "K.clear_session()\n"
      ],
      "metadata": {
        "id": "i8Sf2ACi7bv4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting the model\n",
        "history = model1.fit(train_images, train_labels, batch_size = 15, validation_data=(val_images, val_labels), epochs=5)\n"
      ],
      "metadata": {
        "id": "8w3TF8PA_gaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6087d016-7cfd-4291-cf01-a1c4687180be"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.2340 - accuracy: 0.8911 - val_loss: 0.0673 - val_accuracy: 0.9778\n",
            "Epoch 2/5\n",
            "14/14 [==============================] - 15s 1s/step - loss: 0.0985 - accuracy: 0.9752 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "14/14 [==============================] - 23s 2s/step - loss: 0.1091 - accuracy: 0.9752 - val_loss: 0.0503 - val_accuracy: 0.9778\n",
            "Epoch 4/5\n",
            "14/14 [==============================] - 20s 1s/step - loss: 0.1103 - accuracy: 0.9802 - val_loss: 0.2288 - val_accuracy: 0.9333\n",
            "Epoch 5/5\n",
            "14/14 [==============================] - 22s 2s/step - loss: 0.0973 - accuracy: 0.9604 - val_loss: 0.0893 - val_accuracy: 0.9778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images = []\n",
        "\n",
        "for file in test_files:\n",
        "\n",
        "    img = load_img(file, target_size=(img_width, img_height))\n",
        "    img_array = img_to_array(img)\n",
        "    test_images.append(img_array)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "test_images = np.array(test_images)\n",
        "test_labels = np.array(test_labels)\n"
      ],
      "metadata": {
        "id": "WioKfEOw82SK"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "test_loss, test_accuracy = model1.evaluate(test_images, test_labels, batch_size=10)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy*100)\n",
        "\n",
        "# Predicting classes\n",
        "predictions = model1.predict(test_images)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Generating true classes\n",
        "true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Generating class labels\n",
        "class_labels = [\"laminar\", \"turbulent\"]\n",
        "\n",
        "# Generating classification report\n",
        "print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJQ9R7A18whv",
        "outputId": "405231a3-4fa3-4966-95c3-6b6990100d23"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 235ms/step - loss: 0.0934 - accuracy: 0.9545\n",
            "Test Loss: 0.0933522954583168\n",
            "Test Accuracy: 95.45454382896423\n",
            "2/2 [==============================] - 1s 239ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     laminar       1.00      0.90      0.95        21\n",
            "   turbulent       0.92      1.00      0.96        23\n",
            "\n",
            "    accuracy                           0.95        44\n",
            "   macro avg       0.96      0.95      0.95        44\n",
            "weighted avg       0.96      0.95      0.95        44\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential([\n",
        "        Conv2D(96, (11,11), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "        MaxPooling2D(3,3),\n",
        "        Conv2D(256, (5,5), padding='same', activation='relu'),\n",
        "        Conv2D(256, (3,3), padding='same', activation='relu'),\n",
        "        MaxPooling2D(3,3),\n",
        "        Conv2D(384, (3,3), padding='same', activation='relu'),\n",
        "        Conv2D(384, (3,3), padding='same', activation='relu'),\n",
        "        MaxPooling2D(3,3),\n",
        "        Flatten(),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# compiling the model\n",
        "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# print(model.summary())\n"
      ],
      "metadata": {
        "id": "Oaegs8EL-yJz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fitting the model\n",
        "history = model2.fit(train_images, train_labels, batch_size = 10, validation_data=(val_images, val_labels), epochs=4)\n"
      ],
      "metadata": {
        "id": "RME_aZdtN7uJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4784df-8281-4735-b66c-518f96494aa8"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "21/21 [==============================] - 139s 6s/step - loss: 57.5256 - accuracy: 0.4950 - val_loss: 0.6932 - val_accuracy: 0.4667\n",
            "Epoch 2/4\n",
            "21/21 [==============================] - 124s 6s/step - loss: 0.6966 - accuracy: 0.4406 - val_loss: 0.6922 - val_accuracy: 0.5333\n",
            "Epoch 3/4\n",
            "21/21 [==============================] - 128s 6s/step - loss: 0.7055 - accuracy: 0.4851 - val_loss: 0.6864 - val_accuracy: 0.6222\n",
            "Epoch 4/4\n",
            "21/21 [==============================] - 127s 6s/step - loss: 0.6851 - accuracy: 0.5792 - val_loss: 0.5060 - val_accuracy: 0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "test_loss, test_accuracy = model2.evaluate(test_images, test_labels, batch_size=10)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy*100)\n",
        "\n",
        "# Predicting classes\n",
        "predictions = model2.predict(test_images)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Generating true classes\n",
        "true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Generating class labels\n",
        "class_labels = [\"laminar\", \"turbulent\"]\n",
        "\n",
        "# Generating classification report\n",
        "print(classification_report(true_classes, predicted_classes, target_names=class_labels))\n"
      ],
      "metadata": {
        "id": "b0sYs6UHOGwP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7e468a-5c38-48c2-ebe6-141699f29f47"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 8s 2s/step - loss: 0.5613 - accuracy: 0.7955\n",
            "Test Loss: 0.5612590312957764\n",
            "Test Accuracy: 79.54545617103577\n",
            "2/2 [==============================] - 5s 1s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     laminar       0.70      1.00      0.82        21\n",
            "   turbulent       1.00      0.61      0.76        23\n",
            "\n",
            "    accuracy                           0.80        44\n",
            "   macro avg       0.85      0.80      0.79        44\n",
            "weighted avg       0.86      0.80      0.79        44\n",
            "\n"
          ]
        }
      ]
    }
  ]
}